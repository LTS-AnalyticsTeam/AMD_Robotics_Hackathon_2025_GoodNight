#!/usr/bin/python
# -*- coding: utf-8 -*-
"""Streamlitãƒ™ãƒ¼ã‚¹ã®SO-101åˆ¶å¾¡ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰.

ã‚«ãƒ¡ãƒ©ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨LeRobotæ¨è«–ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®é–‹å§‹ãƒ»åœæ­¢ã‚’Web UIã‹ã‚‰æ“ä½œã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚

Todo:
    * ä¾‹å¤–ç™ºç”Ÿæ™‚ã«è©³ç´°ãƒ­ã‚°ã‚’è¡¨ç¤ºã™ã‚‹ãƒ“ãƒ¥ãƒ¼ã‚’è¿½åŠ ã™ã‚‹ã€‚
    * ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ã‚’WebSocketç­‰ã§éåŒæœŸåŒ–ã™ã‚‹ã€‚

"""

from __future__ import annotations

import sys
import threading
from dataclasses import dataclass
from pathlib import Path
from typing import Optional

import av
import cv2
import numpy as np
import streamlit as st
from streamlit_webrtc import VideoProcessorBase, WebRtcMode, webrtc_streamer

sys.path.insert(0, str(Path(__file__).parent.parent))

from act_inference_control import run_eval_inference

# ã‚«ãƒ¡ãƒ©è¨­å®š
CAMERA_ID = 6
PREVIEW_WIDTH = 2180
PREVIEW_HEIGHT = 1440


@dataclass
class RobotInferenceConfig:
    """ãƒ­ãƒœãƒƒãƒˆæ¨è«–ã®è¨­å®šã‚’ä¿æŒã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹.

    ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«å¤‰æ›´ãŒå¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã€å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç®¡ç†ã™ã‚‹ã€‚

    Attributes:
        model_id (str): Hugging Face Hubä¸Šã®ãƒ¢ãƒ‡ãƒ«IDã€‚
        meta_repo_id (str): ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒªãƒã‚¸ãƒˆãƒªIDã€‚
        task (str): ã‚¿ã‚¹ã‚¯ã®èª¬æ˜æ–‡ã€‚
        dataset_id (Optional[str]): è©•ä¾¡çµæœã‚’ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆIDï¼ˆä¿å­˜ã—ãªã„å ´åˆã¯Noneï¼‰ã€‚
        num_episodes (int): å®Ÿè¡Œã™ã‚‹ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ã€‚
        fps (int): åˆ¶å¾¡ãŠã‚ˆã³è¨˜éŒ²ã®FPSã€‚
        episode_time_s (int): 1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®åˆ¶å¾¡æ™‚é–“ï¼ˆç§’ï¼‰ã€‚
        left_arm_port (str): å·¦è…•ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ã®ã‚·ãƒªã‚¢ãƒ«ãƒãƒ¼ãƒˆã€‚
        right_arm_port (str): å³è…•ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ã®ã‚·ãƒªã‚¢ãƒ«ãƒãƒ¼ãƒˆã€‚
        front_cam_index (int): æ­£é¢ã‚«ãƒ¡ãƒ©ã®ãƒ‡ãƒã‚¤ã‚¹IDã€‚
        above_cam_index (int): ä¸Šéƒ¨ã‚«ãƒ¡ãƒ©ã®ãƒ‡ãƒã‚¤ã‚¹IDã€‚
        device (str): ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹ï¼ˆ"cuda" ã¾ãŸã¯ "cpu"ï¼‰ã€‚
        use_videos (bool): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜æ™‚ã«å‹•ç”»ã‚’å«ã‚ã‚‹ã‹ã©ã†ã‹ã€‚
        image_writer_threads (int): å‹•ç”»ä¿å­˜ã®ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã€‚
        push_to_hub (bool): å®Ÿè¡Œçµ‚äº†æ™‚ã«Hubã¸pushã™ã‚‹ã‹ã©ã†ã‹ã€‚
        save_dataset (bool): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹ã€‚Falseã®å ´åˆã¯æ¨è«–ã®ã¿ã€‚

    """

    model_id: str
    meta_repo_id: str
    task: str
    dataset_id: Optional[str] = None
    num_episodes: int = 1
    fps: int = 30
    episode_time_s: int = 60
    left_arm_port: str = "/dev/ttyACM2"
    right_arm_port: str = "/dev/ttyACM3"
    front_cam_index: int = 4
    above_cam_index: int = 6
    device: str = "cuda"
    use_videos: bool = False
    image_writer_threads: int = 4
    push_to_hub: bool = False
    save_dataset: bool = False


### å„ã‚¿ã‚¹ã‚¯ã®ãƒ¢ãƒ‡ãƒ«è¨­å®š
# å¸ƒå›£ã‚’ã‹ã‘ã‚‹ã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«
DRAPE_BLANKET_CONFIG = RobotInferenceConfig(
    model_id="lt-s/AMD_hackathon2025_blanket_act_drape_004000",
    meta_repo_id="lt-s/AMD_hackathon_drape_blanket",
    task="Grab the red grip to unfold the blanket, then gently place it.",
)

# å¸ƒå›£ã‚’å¤–ã™ã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«
REMOVE_BLANKET_CONFIG = RobotInferenceConfig(
    model_id="lt-s/AMD_hackathon2025_blanket_act_fold_001600",
    meta_repo_id="lt-s/AMD_hackathon_fold_blanket",
    task="Lift the blanket from the doll's neck. Fold the blanket and place it gently next to the doll.",
)


def _open_capture(camera_id: int) -> cv2.VideoCapture:
    """æŒ‡å®šã•ã‚ŒãŸIDã§OpenCVã®VideoCaptureã‚’åˆæœŸåŒ–ã™ã‚‹.

    Args:
        camera_id (int): åˆ©ç”¨ã™ã‚‹ã‚«ãƒ¡ãƒ©ã®ãƒ‡ãƒã‚¤ã‚¹IDã€‚

    Returns:
        cv2.VideoCapture: åˆæœŸåŒ–æ¸ˆã¿ã®ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã€‚

    Raises:
        RuntimeError: ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã§ããªã‹ã£ãŸå ´åˆã«ç™ºç”Ÿã€‚

    """

    capture = cv2.VideoCapture(camera_id)
    capture.set(cv2.CAP_PROP_FRAME_WIDTH, PREVIEW_WIDTH)
    capture.set(cv2.CAP_PROP_FRAME_HEIGHT, PREVIEW_HEIGHT)
    if not capture.isOpened():
        raise RuntimeError(f"ã‚«ãƒ¡ãƒ©ID {camera_id} ã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã§ãã¾ã›ã‚“ã€‚")
    return capture


class CameraVideoProcessor(VideoProcessorBase):
    """WebRTCç”¨ã«OpenCVã®ã‚«ãƒ¡ãƒ©ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æä¾›ã™ã‚‹ãƒ—ãƒ­ã‚»ãƒƒã‚µ."""

    def __init__(self) -> None:
        self.capture: Optional[cv2.VideoCapture] = None

    def _ensure_capture(self) -> cv2.VideoCapture:
        if self.capture is None or not self.capture.isOpened():
            if self.capture is not None:
                self.capture.release()
            self.capture = _open_capture(CAMERA_ID)
        return self.capture

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        """WebRTCçµŒç”±ã§é€ä¿¡ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç”Ÿæˆã™ã‚‹."""

        try:
            capture = self._ensure_capture()
            ret, image = capture.read()
        except RuntimeError:
            self.capture = None
            ret, image = False, None

        if not ret or image is None:
            fallback = np.zeros((PREVIEW_HEIGHT, PREVIEW_WIDTH, 3), dtype=np.uint8)
            return av.VideoFrame.from_ndarray(fallback, format="bgr24")

        resized = cv2.resize(image, (PREVIEW_WIDTH, PREVIEW_HEIGHT), interpolation=cv2.INTER_AREA)
        return av.VideoFrame.from_ndarray(resized, format="bgr24")

    def __del__(self) -> None:
        if self.capture is not None and self.capture.isOpened():
            self.capture.release()


def _init_session_state() -> None:
    """Streamlitã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«å¿…è¦ãªã‚­ãƒ¼ã‚’åˆæœŸåŒ–ã™ã‚‹.

    Returns:
        None: è¿”ã‚Šå€¤ã¯ãªã„ã€‚

    """

    if "inference_thread" not in st.session_state:
        st.session_state.inference_thread = None
    if "inference_stop_event" not in st.session_state:
        st.session_state.inference_stop_event = None
    if "inference_running" not in st.session_state:
        st.session_state.inference_running = False


def _start_inference(config: RobotInferenceConfig) -> None:
    """æŒ‡å®šã•ã‚ŒãŸè¨­å®šã§æ¨è«–ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§é–‹å§‹ã™ã‚‹.

    Args:
        config (RobotInferenceConfig): æ¨è«–è¨­å®šã€‚

    Returns:
        None: è¿”ã‚Šå€¤ã¯ãªã„ã€‚

    """

    if st.session_state.inference_running:
        st.warning("æ—¢ã«æ¨è«–ãŒå®Ÿè¡Œä¸­ã§ã™ã€‚")
        return

    stop_event = threading.Event()

    def inference_wrapper():
        """æ¨è«–ã‚’å®Ÿè¡Œã—ã€åœæ­¢ã‚¤ãƒ™ãƒ³ãƒˆã«å¯¾å¿œã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°."""
        try:
            run_eval_inference(
                model_id=config.model_id,
                dataset_id=config.dataset_id,
                task_description=config.task,
                num_episodes=config.num_episodes,
                fps=config.fps,
                episode_time_s=config.episode_time_s,
                left_arm_port=config.left_arm_port,
                right_arm_port=config.right_arm_port,
                front_cam_index=config.front_cam_index,
                above_cam_index=config.above_cam_index,
                device=config.device,
                use_videos=config.use_videos,
                image_writer_threads=config.image_writer_threads,
                push_to_hub=config.push_to_hub,
                save_dataset=config.save_dataset,
                meta_repo_id=config.meta_repo_id,
                stop_event=stop_event,
            )
        except Exception as e:
            st.error(f"æ¨è«–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        finally:
            st.session_state.inference_running = False

    inference_thread = threading.Thread(
        target=inference_wrapper,
        daemon=True,
        name="robot-inference",
    )
    st.session_state.inference_stop_event = stop_event
    st.session_state.inference_thread = inference_thread
    st.session_state.inference_running = True
    inference_thread.start()


def _stop_inference() -> None:
    """æ¨è«–ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’åœæ­¢ã—ã€å¾Œå‡¦ç†ã‚’è¡Œã†.

    Returns:
        None: è¿”ã‚Šå€¤ã¯ãªã„ã€‚

    """

    if not st.session_state.inference_running:
        st.info("æ¨è«–ã¯å®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return

    stop_event: Optional[threading.Event] = st.session_state.inference_stop_event
    inference_thread: Optional[threading.Thread] = st.session_state.inference_thread
    
    if stop_event is not None:
        stop_event.set()
        st.info("åœæ­¢ã‚·ã‚°ãƒŠãƒ«ã‚’é€ä¿¡ã—ã¾ã—ãŸã€‚ãƒ­ãƒœãƒƒãƒˆãŒå®‰å…¨ã«åœæ­¢ã™ã‚‹ã¾ã§ãŠå¾…ã¡ãã ã•ã„...")
    
    if inference_thread is not None and inference_thread.is_alive():
        inference_thread.join(timeout=10)
        if inference_thread.is_alive():
            st.warning("æ¨è«–ã‚¹ãƒ¬ãƒƒãƒ‰ãŒ10ç§’ä»¥å†…ã«åœæ­¢ã—ã¾ã›ã‚“ã§ã—ãŸã€‚å¼·åˆ¶çµ‚äº†ã—ã¾ã™ã€‚")
    
    st.session_state.inference_stop_event = None
    st.session_state.inference_thread = None
    st.session_state.inference_running = False
    st.success("æ¨è«–ã‚’åœæ­¢ã—ã¾ã—ãŸã€‚")


def main() -> None:
    """Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ.

    Returns:
        None: è¿”ã‚Šå€¤ã¯ãªã„ã€‚

    """

    st.set_page_config(page_title="Robot Control", layout="wide")
    _init_session_state()

    st.title("ğŸ¤– TEAM13_LTS Robotics_Teamï¼šGoodNight")

    st.subheader("ğŸ“¹ ãƒ©ã‚¤ãƒ–ãƒ“ãƒ¥ãƒ¼")

    webrtc_ctx = webrtc_streamer(
        key="camera-preview",
        mode=WebRtcMode.SENDONLY,
        video_processor_factory=CameraVideoProcessor,
        media_stream_constraints={"video": {"width": PREVIEW_WIDTH, "height": PREVIEW_HEIGHT}, "audio": False},
    )

    st.subheader("ğŸ® åˆ¶å¾¡ãƒœã‚¿ãƒ³")

    col1, col2 = st.columns(2)
    with col1:
        if st.button("â–¶ï¸ å¸ƒå›£æ›ã‘é–‹å§‹", key="drape_start", use_container_width=True):
            _start_inference(DRAPE_BLANKET_CONFIG)

    with col2:
        if st.button("ğŸ”„ å¸ƒå›£å¤–ã—é–‹å§‹", key="remove_start", use_container_width=True):
            _start_inference(REMOVE_BLANKET_CONFIG)

    if st.button("â¹ï¸ åœæ­¢", key="inference_stop", use_container_width=True):
        _stop_inference()

    inference_state = "å®Ÿè¡Œä¸­" if st.session_state.inference_running else "å¾…æ©Ÿä¸­"
    st.info(f"ğŸ¤– æ¨è«–: {inference_state}")

    st.divider()
    st.caption("LeRobot Control Interface v1.0")


if __name__ == "__main__":
    main()
